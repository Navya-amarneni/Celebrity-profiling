{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S9bIpKoI7IR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDifz2n289l3"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from keras.models import Model\r\n",
        "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\r\n",
        "from keras.optimizers import RMSprop\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing import sequence\r\n",
        "from keras.utils import to_categorical\r\n",
        "from keras.callbacks import EarlyStopping\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn7r3HNOI7Iw"
      },
      "source": [
        "from sklearn.feature_selection import chi2\n",
        "\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import sys\n",
        "import numpy as n\n",
        "import pdb\n",
        "from sklearn.metrics import make_scorer, f1_score, accuracy_score, recall_score, precision_score, classification_report, precision_recall_fscore_support\n",
        "from sklearn.ensemble  import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import make_scorer, f1_score, accuracy_score, recall_score, precision_score, classification_report, precision_recall_fscore_support\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.ensemble  import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.utils import shuffle\n",
        "import codecs\n",
        "import operator\n",
        "import sklearn\n",
        "from collections import defaultdict\n",
        "from nltk.tokenize import TweetTokenizer\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEcxOPgCI7JZ"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6ZqRtTpI7JZ"
      },
      "source": [
        "\n",
        "from keras.callbacks import EarlyStopping\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpS_E1SqI7Ja"
      },
      "source": [
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import SimpleRNN\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Bidirectional"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IimP6jwRI7Jb"
      },
      "source": [
        "df=pd.read_csv(\"celebrity_datas.csv\",usecols=['text','gender'])\r\n",
        "df.text=df.text.astype(str)\r\n",
        "df.gender=df.gender.astype(str)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXNylBTXI7J6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9eb6fe8d-83ed-4486-e75f-6c92999252b4"
      },
      "source": [
        "X = df.text\r\n",
        "Y =  df.gender\r\n",
        "le = LabelEncoder()\r\n",
        "Y = le.fit_transform(Y)\r\n",
        "Y = Y.reshape(-1,1)\r\n",
        "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.15)\r\n",
        "max_words = 1000\r\n",
        "max_len = 150\r\n",
        "tok = Tokenizer(num_words=max_words)\r\n",
        "tok.fit_on_texts(X_train)\r\n",
        "sequences = tok.texts_to_sequences(X_train)\r\n",
        "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\r\n",
        "\r\n",
        "def RNN():\r\n",
        "    inputs = Input(name='inputs',shape=[max_len])\r\n",
        "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\r\n",
        "    layer = LSTM(64)(layer)\r\n",
        "    layer = Dense(256,name='FC1')(layer)\r\n",
        "    layer = Activation('relu')(layer)\r\n",
        "    layer = Dropout(0.5)(layer)\r\n",
        "    layer = Dense(1,name='out_layer')(layer)\r\n",
        "    layer = Activation('sigmoid')(layer)\r\n",
        "    model = Model(inputs=inputs,outputs=layer)\r\n",
        "    return model\r\n",
        "\r\n",
        "model = RNN()\r\n",
        "model.summary()\r\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\r\n",
        "\r\n",
        "model.fit(sequences_matrix,Y_train,batch_size=128,epochs=10,\r\n",
        "          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\r\n",
        "\r\n",
        "test_sequences = tok.texts_to_sequences(X_test)\r\n",
        "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\r\n",
        "\r\n",
        "accr = model.evaluate(test_sequences_matrix,Y_test)\r\n",
        "\r\n",
        "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inputs (InputLayer)          [(None, 150)]             0         \n",
            "_________________________________________________________________\n",
            "embedding_4 (Embedding)      (None, 150, 50)           50000     \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 64)                29440     \n",
            "_________________________________________________________________\n",
            "FC1 (Dense)                  (None, 256)               16640     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "out_layer (Dense)            (None, 1)                 257       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 96,337\n",
            "Trainable params: 96,337\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "180/180 [==============================] - 34s 179ms/step - loss: 0.6007 - accuracy: 0.7143 - val_loss: 0.5104 - val_accuracy: 0.7695\n",
            "Epoch 2/10\n",
            "180/180 [==============================] - 32s 178ms/step - loss: 0.4961 - accuracy: 0.7636 - val_loss: 0.4929 - val_accuracy: 0.7634\n",
            "Epoch 3/10\n",
            "180/180 [==============================] - 32s 178ms/step - loss: 0.4594 - accuracy: 0.7916 - val_loss: 0.4889 - val_accuracy: 0.7719\n",
            "Epoch 4/10\n",
            "180/180 [==============================] - 32s 179ms/step - loss: 0.4580 - accuracy: 0.7862 - val_loss: 0.4819 - val_accuracy: 0.7743\n",
            "Epoch 5/10\n",
            "180/180 [==============================] - 33s 182ms/step - loss: 0.4513 - accuracy: 0.7881 - val_loss: 0.5383 - val_accuracy: 0.7745\n",
            "159/159 [==============================] - 3s 21ms/step - loss: 0.5381 - accuracy: 0.7730\n",
            "Test set\n",
            "  Loss: 0.538\n",
            "  Accuracy: 0.773\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "r7NiSl0HZAJq",
        "outputId": "0ee56fb2-2f3c-4b4f-a569-9ba63e7f55d2"
      },
      "source": [
        "df=pd.read_csv(\"celebrity_datas.csv\",usecols=['text','occupation'])\r\n",
        "df.text=df.text.astype(str)\r\n",
        "df.occupation=df.occupation.astype(str)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-96593c33cc03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"celebrity_datas.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'occupation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moccupation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moccupation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHmLIKzGD9PT"
      },
      "source": [
        "df.loc[df['occupation'] == 'science', 'occupation'] = 'politics'\r\n",
        "df.loc[df['occupation'] == 'manager', 'occupation'] = 'politics'\r\n",
        "df.loc[df['occupation'] == 'professional', 'occupation'] = 'politics'\r\n",
        "df.loc[df['occupation'] == 'religious', 'occupation'] = 'politics'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oo9_XdcuEAdd",
        "outputId": "cb7a8955-873e-435d-e73b-2a7e36b6d94d"
      },
      "source": [
        "df.occupation.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sports       13481\n",
              "performer     9899\n",
              "creator       5475\n",
              "politics      4981\n",
              "Name: occupation, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeZfH_4AZxo9",
        "outputId": "0c68c082-1def-4d90-9ba2-4ddb5e36ef69"
      },
      "source": [
        "X = df.text\r\n",
        "Y =  df.occupation\r\n",
        "le = LabelEncoder()\r\n",
        "Y = le.fit_transform(Y)\r\n",
        "Y = Y.reshape(-1,1)\r\n",
        "\r\n",
        "\r\n",
        "MAX_NB_WORDS = 1000\r\n",
        "# Max number of words in each complaint.\r\n",
        "MAX_SEQUENCE_LENGTH = 250\r\n",
        "# This is fixed.\r\n",
        "EMBEDDING_DIM = 100\r\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\r\n",
        "tokenizer.fit_on_texts(df['text'].values)\r\n",
        "word_index = tokenizer.word_index\r\n",
        "print('Found %s unique tokens.' % len(word_index))\r\n",
        "\r\n",
        "X = tokenizer.texts_to_sequences(df['text'].values)\r\n",
        "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\r\n",
        "print('Shape of data tensor:', X.shape)\r\n",
        "\r\n",
        "Y = pd.get_dummies(df['occupation']).values\r\n",
        "print('Shape of label tensor:', Y.shape)\r\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.15, random_state = 42)\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\r\n",
        "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\r\n",
        "model.add(Dense(4, activation='softmax'))\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "epochs = 7\r\n",
        "batch_size = 128\r\n",
        "\r\n",
        "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.15,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\r\n",
        "\r\n",
        "accr = model.evaluate(X_test,Y_test)\r\n",
        "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 388971 unique tokens.\n",
            "Shape of data tensor: (33836, 250)\n",
            "Shape of label tensor: (33836, 4)\n",
            "Epoch 1/7\n",
            "191/191 [==============================] - 138s 707ms/step - loss: 1.2461 - accuracy: 0.4328 - val_loss: 0.9317 - val_accuracy: 0.6342\n",
            "Epoch 2/7\n",
            "191/191 [==============================] - 135s 709ms/step - loss: 0.9459 - accuracy: 0.6295 - val_loss: 0.9036 - val_accuracy: 0.6472\n",
            "Epoch 3/7\n",
            "191/191 [==============================] - 135s 708ms/step - loss: 0.9166 - accuracy: 0.6487 - val_loss: 0.9091 - val_accuracy: 0.6437\n",
            "Epoch 4/7\n",
            "191/191 [==============================] - 135s 707ms/step - loss: 0.8751 - accuracy: 0.6619 - val_loss: 0.8747 - val_accuracy: 0.6676\n",
            "Epoch 5/7\n",
            "191/191 [==============================] - 135s 706ms/step - loss: 0.8490 - accuracy: 0.6784 - val_loss: 0.8824 - val_accuracy: 0.6637\n",
            "Epoch 6/7\n",
            "191/191 [==============================] - 135s 706ms/step - loss: 0.8400 - accuracy: 0.6803 - val_loss: 0.8758 - val_accuracy: 0.6634\n",
            "Epoch 7/7\n",
            "191/191 [==============================] - 135s 707ms/step - loss: 0.8431 - accuracy: 0.6836 - val_loss: 0.8843 - val_accuracy: 0.6548\n",
            "159/159 [==============================] - 6s 38ms/step - loss: 0.8975 - accuracy: 0.6562\n",
            "Test set\n",
            "  Loss: 0.897\n",
            "  Accuracy: 0.656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GwnNLPaqOgj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "0545db14-b1cb-47ed-c7ac-a663eb47b746"
      },
      "source": [
        "df=pd.read_csv(\"celebrity_datas.csv\",usecols=['text','fame'])\r\n",
        "df.text=df.text.astype(str)\r\n",
        "df.fame=df.fame.astype(str)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-a64dc3a60a53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"celebrity_datas.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'fame'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfame\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2157\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2158\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at row 27136"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0Dl_F5UqJeG",
        "outputId": "95c7f73b-fcfa-474f-ad41-e4a0a8728632"
      },
      "source": [
        "X = df.text\r\n",
        "Y =  df.fame\r\n",
        "le = LabelEncoder()\r\n",
        "Y = le.fit_transform(Y)\r\n",
        "Y = Y.reshape(-1,1)\r\n",
        "\r\n",
        "df.fame.value_counts()\r\n",
        "\r\n",
        "MAX_NB_WORDS = 1000\r\n",
        "# Max number of words in each complaint.\r\n",
        "MAX_SEQUENCE_LENGTH = 250\r\n",
        "# This is fixed.\r\n",
        "EMBEDDING_DIM = 100\r\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\r\n",
        "tokenizer.fit_on_texts(df['text'].values)\r\n",
        "word_index = tokenizer.word_index\r\n",
        "print('Found %s unique tokens.' % len(word_index))\r\n",
        "\r\n",
        "X = tokenizer.texts_to_sequences(df['text'].values)\r\n",
        "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\r\n",
        "print('Shape of data tensor:', X.shape)\r\n",
        "\r\n",
        "Y = pd.get_dummies(df['fame']).values\r\n",
        "print('Shape of label tensor:', Y.shape)\r\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.15, random_state = 42)\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\r\n",
        "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\r\n",
        "model.add(Dense(3, activation='softmax'))\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "epochs = 10\r\n",
        "batch_size = 128\r\n",
        "\r\n",
        "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.15,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\r\n",
        "\r\n",
        "accr = model.evaluate(X_test,Y_test)\r\n",
        "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 388971 unique tokens.\n",
            "Shape of data tensor: (33836, 250)\n",
            "Shape of label tensor: (33836, 3)\n",
            "Epoch 1/10\n",
            "191/191 [==============================] - 150s 766ms/step - loss: 0.7772 - accuracy: 0.7292 - val_loss: 0.6736 - val_accuracy: 0.7487\n",
            "Epoch 2/10\n",
            "191/191 [==============================] - 145s 759ms/step - loss: 0.6695 - accuracy: 0.7454 - val_loss: 0.6611 - val_accuracy: 0.7459\n",
            "Epoch 3/10\n",
            "191/191 [==============================] - 145s 759ms/step - loss: 0.6499 - accuracy: 0.7448 - val_loss: 0.6747 - val_accuracy: 0.7427\n",
            "Epoch 4/10\n",
            "191/191 [==============================] - 145s 758ms/step - loss: 0.6273 - accuracy: 0.7508 - val_loss: 0.6671 - val_accuracy: 0.7404\n",
            "Epoch 5/10\n",
            "191/191 [==============================] - 145s 761ms/step - loss: 0.6217 - accuracy: 0.7466 - val_loss: 0.6692 - val_accuracy: 0.7448\n",
            "159/159 [==============================] - 7s 42ms/step - loss: 0.6726 - accuracy: 0.7447\n",
            "Test set\n",
            "  Loss: 0.673\n",
            "  Accuracy: 0.745\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vr85uXtz0vvI"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRQqRRHG5mIT"
      },
      "source": [
        "df=pd.read_csv(\"celebrity_datas.csv\",usecols=['text','birthyear'])\r\n",
        "df.text=df.text.astype(str)\r\n",
        "df.birthyear=df.birthyear.astype(str)\r\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNHbUtmjzXVS",
        "outputId": "2535b558-70a3-49f0-b911-4143ebd98c09"
      },
      "source": [
        "X = df.text\r\n",
        "Y =  df.birthyear\r\n",
        "le = LabelEncoder()\r\n",
        "Y = le.fit_transform(Y)\r\n",
        "Y = Y.reshape(-1,1)\r\n",
        "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2)\r\n",
        "max_words = 100\r\n",
        "max_len = 150\r\n",
        "tok = Tokenizer(num_words=max_words, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',lower=True)\r\n",
        "tok.fit_on_texts(X_train)\r\n",
        "sequences = tok.texts_to_sequences(X_train)\r\n",
        "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\r\n",
        "\r\n",
        "def RNN():\r\n",
        "    inputs = Input(name='inputs',shape=[max_len])\r\n",
        "    layer = Embedding(max_words,100,input_length=max_len)(inputs)\r\n",
        "    layer = LSTM(64)(layer)\r\n",
        "    layer = Dense(256,name='FC1')(layer)\r\n",
        "    layer = Activation('softmax')(layer)\r\n",
        "    \r\n",
        "    layer = Dense(1,name='out_layer')(layer)\r\n",
        "    layer = Activation('sigmoid')(layer)\r\n",
        "    model = Model(inputs=inputs,outputs=layer)\r\n",
        "    return model\r\n",
        "\r\n",
        "model = RNN()\r\n",
        "model.summary()\r\n",
        "model.compile(loss='categorical_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])\r\n",
        "\r\n",
        "model.fit(sequences_matrix,Y_train,batch_size=128,epochs=10,\r\n",
        "          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\r\n",
        "\r\n",
        "test_sequences = tok.texts_to_sequences(X_test)\r\n",
        "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\r\n",
        "\r\n",
        "accr = model.evaluate(test_sequences_matrix,Y_test)\r\n",
        "\r\n",
        "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\r\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inputs (InputLayer)          [(None, 150)]             0         \n",
            "_________________________________________________________________\n",
            "embedding_2 (Embedding)      (None, 150, 100)          10000     \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 64)                42240     \n",
            "_________________________________________________________________\n",
            "FC1 (Dense)                  (None, 256)               16640     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "out_layer (Dense)            (None, 1)                 257       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 69,137\n",
            "Trainable params: 69,137\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "170/170 [==============================] - 46s 260ms/step - loss: 0.0000e+00 - accuracy: 0.3467 - val_loss: 0.0000e+00 - val_accuracy: 0.3661\n",
            "Epoch 2/10\n",
            "170/170 [==============================] - 44s 256ms/step - loss: 0.0000e+00 - accuracy: 0.3644 - val_loss: 0.0000e+00 - val_accuracy: 0.3661\n",
            "212/212 [==============================] - 5s 22ms/step - loss: 0.0000e+00 - accuracy: 0.3613\n",
            "Test set\n",
            "  Loss: 0.000\n",
            "  Accuracy: 0.361\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}